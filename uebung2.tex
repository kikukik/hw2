\documentclass[
ngerman,
]{tudaexercise}

\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{array}
\usepackage{gauss}
\usepackage{graphicx}
\usepackage{lipsum}% For example text
\usepackage{listings}
\graphicspath{ {./images/} }
\newcommand{\sni}{\sum_{i=1}^{n}}

\begin{document}
	
	\title[Uebung]{SML: Exercise 2}
	\author{Rinor Cakaj, Patrick Nowak}
	\term{Summer Term 2020}
	\sheetnumber{1}
	
	\maketitle
	
	\begin{task}{Density Estimation}
		We are given data C1 and C2, which we suppose to be generated by 2D-Gaussians with parameters ${\mu_1,\Sigma_1}$ and ${\mu_2,\Sigma_2}$, respectively.
		\begin{subtask}
			Assume we are given iid. datapoints $x_i, i=1,..,n$ which are generated by a 2D-Gaussian. Following the max-likelihood principle, we maximize the log-likelihood function
			\begin{align*}
				l(\mu,\Sigma,x_1,...,x_n)=\ln(\prod_{i=1}^n p(x_i|\mu,\Sigma))=\sni\ln(p(x_i|\mu,\Sigma))
			\end{align*}
			for the Gaussian probability density
			\begin{align} p(x|\mu,\Sigma)=\frac{1}{\sqrt{(2\pi)^k|\Sigma|}}\exp\left( -\frac{1}{2}(x-\mu)^T\Sigma(x-\mu)\right)\;.
			\end{align}
			We receive
			\begin{align}
			l(\mu,\Sigma):=l(\mu,\Sigma,x_1,...,x_n)&=\sni\left( -\frac{k}{2}\ln(2\pi)-\frac{1}{2}ln(|\Sigma|)-\frac{1}{2}(x_i-\mu)^T\Sigma(x_i-\mu)\right) \\
			&=-\frac{nk}{2}\ln(2\pi)-\frac{n}{2}ln(|\Sigma|)-\frac{1}{2}\sni (x_i-\mu)^T\Sigma(x_i-\mu)\;\;.\label{lfkt}
			\end{align}
			We compute the derivatives w.r.t. $\mu$ and $\Sigma$ and set them equal to zero. This yields
			\begin{align*}
\frac{d}{d\mu}l(\mu,\Sigma,x_1,...,x_n)&=\frac{d}{d\mu}\; -\frac{1}{2}\sni (x_i-\mu)^T\Sigma (x_i-\mu)\\ &= -\sni \frac{d}{d\mu}\frac{1}{2}(x_i-\mu)^T\Sigma (x_i-\mu)\;\;.
			\end{align*}
			Using the matrix identity $\frac{d}{dw}\frac{w^T Aw}{dw}=2Aw$ which holds if $w$ does not depend on $A$ and if $A$ is symmetric, we get (with $w=(x-\mu), dw=-d\mu$)
			\begin{align*}
			0&\stackrel{!}{=}\frac{d}{d\mu}l(\mu,\Sigma,x_1,...,x_n)\\
			0&\stackrel{!}{=}-\sni \Sigma^{-1}(x_i-\mu)	\;\;.
			\end{align*}
			Finally, we use that $\Sigma^{-1}$ is positive definite, so we can leave it out here and get
			\begin{align*}
				0&\stackrel{!}{=}n\mu-\sni x_i\;\;,
			\end{align*}
			which is solved for the MLE-estimate
			\begin{align}
				\hat{\mu}&=\frac{1}{n}\sni x_i\;\;.
			\end{align}
			Secondly, we need to compute the derivative w.r.t $\Sigma$. To do that, we will need some results from mathematical classes. The following is used without prove:
			\begin{itemize}
				\item Cyclic permutations of a matrix product do not change the trace of it:\begin{align*}
					tr\left[ ABC\right] = tr\left[ CAB\right] 
				\end{align*}
				\item The trace of a scalar is the scalar itself. In particular: the result of a quadratic form $x^T Ax$ is a scalar, such that:\begin{align*}
					x^T Ax=tr\left[ x^TAx\right] =tr\left[ x^TxA\right] 
				\end{align*}
				\item $\frac{d}{dA}tr\left[ AB\right] =B^T$
				\item $\frac{d}{dA}\ln|A|=A^{-T}$
			\end{itemize}
		As a first result of these assumptions, we can show, that\begin{align*}
			\frac{d}{dA}x^T Ax=\frac{d}{dA}tr\left[ x^T xA\right] =\left[ xx^T\right] ^T=xx^T\;.
		\end{align*} 
		We now got the tools to re-write the log-likelihood function in (\ref{lfkt}) to
		\begin{align*}
			l(\mu,\Sigma)&=-\frac{nk}{2}\ln(2\pi)-\frac{n}{2}ln(|\Sigma|)-\frac{1}{2}\sni (x_i-\mu)^T\Sigma(x_i-\mu)\\&=C+\frac{n}{2}ln(|\Sigma^{-1}|)-\frac{1}{2}\sni tr\left[ (x_i-\mu)(x_i-\mu)^T \Sigma^{-1}\right] 
		\end{align*}
		for a constant C, and taking the derivative w.r.t $\Sigma^{-1}$ yields
		\begin{align*}
			\frac{d}{d\Sigma^{-1}}l(\mu,\Sigma)=\frac{n}{2}\Sigma^T-\frac{1}{2}\sni(x_i-\mu)(x_i-\mu)^T
		\end{align*}
		and plugging in $\hat{\mu}$ as an estimation of $\mu$ and setting equal to zero finally gives us
		\begin{align*}
			0&\stackrel{!}{=}\frac{d}{d\Sigma^{-1}}l(\hat{\mu},\Sigma)\\
			0&\stackrel{!}{=}\frac{n}{2}\Sigma^T-\frac{1}{2}\sni(x_i-\hat{\mu})(x_i-\hat{\mu})^T
		\end{align*}
		which is solved for the (biased) MLE estimate
		\begin{align}
			\tilde{\Sigma}=\frac{1}{n}\sni (x_i-\hat{\mu})(x_i-\hat{\mu})^T
		\end{align}
		\end{subtask}
	\begin{subtask}
		We compute the prior probabilities of C1 and C2, using the following python code. We read the number of data points in each class and divide it by the sum of total data points in both classes.
		\begin{lstlisting}[language=Python]
	import numpy as np
	
	link1="../hw2/dataSets/densEst1.txt"
	link2="../hw2/dataSets/densEst2.txt"
	
	def get_lengths():
		l1=0;
		l2=0;
		for line in open(link1):
			l1=l1+1
		for line2 in open(link2):
			l2=l2+1
		return (l1,l2)    
	
	def get_priors(l1,l2):
		p_C1=l1/(l1+l2)
		p_C2=l2/(l1+l2)
		return(p_C1,p_C2)
		\end{lstlisting}
		Calling
		\begin{lstlisting}[language=Python]
	lengths=get_lengths()
	print(get_priors(lengths[0],lengths[1]))
		\end{lstlisting}
		we get the following results for the prior probabilities: p(C1)=0.239 and p(C2)=0.761 .
		\end{subtask}
	\end{task}
	
\end{document}